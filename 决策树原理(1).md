## 决策树原理(1)

### 一 定义

决策树(Decision Tree)是基于树结构来进行决策的,一类常见的机器学习算法。具体分为分类树(用于分类)和回归树(用于回归)

#### 决策树的基本概念

​	结点：树的结点包含一个数据域和 m 个指针域用来指向它的子树 ，结点的分类为、根结点、叶子结点、内部结点，结点拥有子树的个数被称为结点的度树中各个结点度的最大值被称为树的度

​	根节点：一棵树的根结点只有一个

​	叶子节点：度为 0 的结点被称为叶子结点，不能指向任何子树

​	内部节点：除了根结点和叶子结点以外的结点都被称为内部结点

![image-20230818192035760](C:\Users\jsd2002\Desktop\第三期\img\image-20230818192035760.png)

如图，a为根节点，b、c、d为内部节点，ghief为叶子节点

#### 决策树的策略

基本思想：希望选择结点的基本思想:希望决策树的分支结点所包含的样本尽可能的属于同一类别,即结点的“纯度”
越高越好。

#### 基本流程

1 在每个结点处寻找一个“划分”属性自根至叶的梯归过程

2 停止划分的条件︰

​	1) 当前结点包含的样本全属于同一类别

​	2) 达到事先规定的最大深度

​	3) 结点包含的样本数小于事先规定的最小个数

​	4) 纯度提升小于事先指定阈值



### 二 集合的纯度

#### 信息嫡(Information entropy)

​	通信中n个信号,信息量定义为 $log_2n$(二进制存储)
但统计中,习惯定义为$lnn = \frac{log_2n}{log_2e}=C*log_2n$，其中C = $ln2$

信息嫡(Information entropy)是度量样本集合纯度的一种指标
若当前样本集合D中第k类样本所占比例为pk (k = 1,2, ... ,K)，则D的信息嫡定义为
$$
Ent(D)=-\sum_{k=1}^{K}p_kln(p_k)
$$
**Ent(D)的值越小,则D的纯度越高。**

